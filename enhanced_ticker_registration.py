#!/usr/bin/env python3
"""
Enhanced Ticker Auto-Registration System
Rozszerzony system automatycznej rejestracji ticker√≥w podczas importu
Automatycznie tworzy tickery, analizuje dane i przygotowuje integracjƒô ze scrapingiem
Autor: GPW Investor System
Data: 2025-06-25
"""

import logging
from sqlalchemy import create_engine, text
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
import json
import os
from dotenv import load_dotenv
import requests
from ticker_manager import TickerManager

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class EnhancedTickerAutoRegistration:
    """Rozszerzony system auto-rejestracji ticker√≥w"""
    
    def __init__(self):
        load_dotenv('.env')
        
        # Konfiguracja PostgreSQL
        self.pg_config = {
            'user': os.getenv('DB_USER'),
            'password': os.getenv('DB_PASSWORD'),
            'host': os.getenv('DB_HOST'),
            'port': os.getenv('DB_PORT'),
            'database': os.getenv('DB_NAME')
        }
        
        # Sprawd≈∫ konfiguracjƒô
        missing_config = [k for k, v in self.pg_config.items() if not v]
        if missing_config:
            raise ValueError(f"Brakuje konfiguracji PostgreSQL: {missing_config}")
        
        # Po≈ÇƒÖczenie PostgreSQL
        db_uri = f"postgresql://{self.pg_config['user']}:{self.pg_config['password']}@{self.pg_config['host']}:{self.pg_config['port']}/{self.pg_config['database']}"
        self.engine = create_engine(db_uri)
        
        # Ticker Manager
        self.ticker_manager = TickerManager()
        
        # Cache informacji o sp√≥≈Çkach
        self.company_info_cache = {}
        
        logger.info("‚úÖ Enhanced Ticker Auto-Registration zainicjalizowany")
    
    def get_company_info(self, ticker: str) -> Dict[str, str]:
        """
        Pr√≥buje uzyskaƒá informacje o sp√≥≈Çce (nazwa, sektor)
        U≈ºywa r√≥≈ºnych ≈∫r√≥de≈Ç: cache, GPW, Stooq, itp.
        """
        ticker_upper = ticker.upper()
        
        # Sprawd≈∫ cache
        if ticker_upper in self.company_info_cache:
            return self.company_info_cache[ticker_upper]
        
        # Domy≈õlne warto≈õci
        company_info = {
            'name': ticker_upper,
            'sector': 'Unknown',
            'market': 'GPW',
            'source': 'default'
        }
        
        # Pr√≥ba 1: Sprawd≈∫ czy ju≈º mamy w bazie
        try:
            with self.engine.connect() as conn:
                result = conn.execute(
                    text("SELECT name, sector FROM companies WHERE ticker = :ticker"),
                    {"ticker": ticker_upper}
                ).fetchone()
                
                if result and result[0] and result[0] != ticker_upper:
                    company_info.update({
                        'name': result[0],
                        'sector': result[1] or 'Unknown',
                        'source': 'database'
                    })
                    self.company_info_cache[ticker_upper] = company_info
                    return company_info
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è B≈ÇƒÖd sprawdzania bazy dla {ticker}: {e}")
        
        # Pr√≥ba 2: GPW API/Scraping (mo≈ºna dodaƒá w przysz≈Ço≈õci)
        # company_info = self._get_from_gpw_api(ticker_upper, company_info)
        
        # Pr√≥ba 3: Mapowanie znanych ticker√≥w
        known_companies = {
            'PKN': {'name': 'PKN Orlen', 'sector': 'Paliwa'},
            'PEO': {'name': 'Polski Koncern Naftowy Orlen', 'sector': 'Paliwa'},
            'CDP': {'name': 'CD Projekt', 'sector': 'Gry komputerowe'},
            'JSW': {'name': 'Jastrzƒôbska Sp√≥≈Çka Wƒôglowa', 'sector': 'G√≥rnictwo'},
            'KGH': {'name': 'KGHM Polska Mied≈∫', 'sector': 'G√≥rnictwo'},
            'LPP': {'name': 'LPP', 'sector': 'Odzie≈º'},
            'ALE': {'name': 'Allegro.eu', 'sector': 'E-commerce'},
            'CCC': {'name': 'CCC', 'sector': 'Odzie≈º'},
            'DNP': {'name': 'Dino Polska', 'sector': 'Handel detaliczny'},
            '11B': {'name': '11 bit studios', 'sector': 'Gry komputerowe'},
            'CDR': {'name': 'CD Projekt RED', 'sector': 'Gry komputerowe'},
            'KRU': {'name': 'Kruk', 'sector': 'Us≈Çugi finansowe'},
            'MBK': {'name': 'mBank', 'sector': 'Bankowo≈õƒá'},
            'PKO': {'name': 'PKO Bank Polski', 'sector': 'Bankowo≈õƒá'},
            'PZU': {'name': 'PZU', 'sector': 'Ubezpieczenia'},
            'LTS': {'name': 'Grupa Lotos', 'sector': 'Paliwa'}
        }
        
        if ticker_upper in known_companies:
            company_info.update(known_companies[ticker_upper])
            company_info['source'] = 'known_mapping'
        
        # Cache rezultat
        self.company_info_cache[ticker_upper] = company_info
        logger.info(f"üìä Info o sp√≥≈Çce {ticker}: {company_info['name']} ({company_info['sector']})")
        
        return company_info
    
    def auto_register_from_import(self, ticker: str, import_stats: Dict) -> Dict[str, Any]:
        """
        Rozszerzona auto-rejestracja podczas importu
        
        Args:
            ticker: Symbol sp√≥≈Çki
            import_stats: Statystyki importu (imported, skipped, errors, etc.)
        
        Returns:
            Dict z wynikami rejestracji
        """
        result = {
            'ticker': ticker.upper(),
            'registered': False,
            'company_created': False,
            'data_source_registered': False,
            'continuity_analyzed': False,
            'company_info': {},
            'continuity_analysis': {},
            'errors': []
        }
        
        try:
            # 1. Uzyskaj informacje o sp√≥≈Çce
            company_info = self.get_company_info(ticker)
            result['company_info'] = company_info
            
            # 2. Zarejestruj ticker z rozszerzonymi informacjami
            success = self.ticker_manager.register_ticker(
                ticker=ticker,
                name=company_info['name'],
                sector=company_info['sector'],
                source_type='historical'
            )
            
            if success:
                result['registered'] = True
                result['company_created'] = True
                logger.info(f"‚úÖ Zarejestrowano sp√≥≈Çkƒô {ticker}: {company_info['name']}")
                
                # 3. Zarejestruj ≈∫r√≥d≈Ço danych z metadanymi
                source_success = self.ticker_manager.register_data_source(
                    ticker=ticker,
                    source_type='historical_import',
                    source_name='txt_files',
                    record_count=import_stats.get('imported', 0),
                    metadata={
                        'import_date': datetime.now().isoformat(),
                        'imported_records': import_stats.get('imported', 0),
                        'skipped_records': import_stats.get('skipped', 0),
                        'error_records': import_stats.get('errors', 0),
                        'source_file': import_stats.get('source_file', 'unknown'),
                        'company_info_source': company_info['source'],
                        'auto_registered': True
                    }
                )
                
                if source_success:
                    result['data_source_registered'] = True
                    
                    # 4. Analizuj ciƒÖg≈Ço≈õƒá danych
                    if import_stats.get('imported', 0) > 0:
                        continuity = self.ticker_manager.analyze_data_continuity(ticker)
                        result['continuity_analysis'] = continuity
                        result['continuity_analyzed'] = True
                        
                        # 5. Przygotuj rekomendacje dla scrapingu
                        scraping_suggestion = self.ticker_manager.suggest_scraping_schedule(ticker)
                        result['scraping_suggestion'] = scraping_suggestion
                        
                        logger.info(f"üìà Analiza {ticker}: {continuity.get('total_records', 0)} rekord√≥w, "
                                  f"ostatnie: {continuity.get('last_date', 'N/A')}")
                        
                        if scraping_suggestion['priority'] == 'high':
                            logger.warning(f"‚ö†Ô∏è {ticker}: {scraping_suggestion['reason']}")
                
            else:
                result['errors'].append("B≈ÇƒÖd rejestracji tickera")
                
        except Exception as e:
            error_msg = f"B≈ÇƒÖd auto-rejestracji {ticker}: {e}"
            result['errors'].append(error_msg)
            logger.error(f"‚ùå {error_msg}")
        
        return result
    
    def analyze_import_batch(self, ticker_results: List[Dict]) -> Dict[str, Any]:
        """Analizuje wyniki importu dla ca≈Çej partii ticker√≥w"""
        analysis = {
            'total_tickers': len(ticker_results),
            'successful_registrations': 0,
            'failed_registrations': 0,
            'new_companies': 0,
            'companies_with_data': 0,
            'high_priority_for_scraping': 0,
            'sectors_summary': {},
            'data_quality_issues': [],
            'recommendations': []
        }
        
        for result in ticker_results:
            if result['registered']:
                analysis['successful_registrations'] += 1
                
                if result['company_created']:
                    analysis['new_companies'] += 1
                
                # Analiza sektor√≥w
                sector = result['company_info'].get('sector', 'Unknown')
                if sector not in analysis['sectors_summary']:
                    analysis['sectors_summary'][sector] = 0
                analysis['sectors_summary'][sector] += 1
                
                # Sprawd≈∫ jako≈õƒá danych
                if result.get('continuity_analyzed'):
                    continuity = result['continuity_analysis']
                    if continuity.get('total_records', 0) > 0:
                        analysis['companies_with_data'] += 1
                    
                    # Problemy z jako≈õciƒÖ
                    if continuity.get('gap_count', 0) > 0:
                        analysis['data_quality_issues'].append({
                            'ticker': result['ticker'],
                            'issue': 'data_gaps',
                            'gap_count': continuity.get('gap_count', 0)
                        })
                    
                    # Priority scrapingu
                    suggestion = result.get('scraping_suggestion', {})
                    if suggestion.get('priority') == 'high':
                        analysis['high_priority_for_scraping'] += 1
            else:
                analysis['failed_registrations'] += 1
        
        # Generuj rekomendacje
        if analysis['high_priority_for_scraping'] > 0:
            analysis['recommendations'].append(
                f"üî• {analysis['high_priority_for_scraping']} ticker√≥w wymaga natychmiastowego scrapingu"
            )
        
        if analysis['data_quality_issues']:
            analysis['recommendations'].append(
                f"‚ö†Ô∏è {len(analysis['data_quality_issues'])} ticker√≥w ma luki w danych historycznych"
            )
        
        logger.info(f"üìä Analiza importu: {analysis['successful_registrations']}/{analysis['total_tickers']} "
                   f"ticker√≥w zarejestrowanych, {analysis['new_companies']} nowych sp√≥≈Çek")
        
        return analysis
    
    def generate_integration_readiness_report(self) -> Dict[str, Any]:
        """Generuje raport gotowo≈õci do integracji danych historycznych ze scrapingiem"""
        try:
            # Pobierz wszystkie tickery
            all_tickers = self.ticker_manager.get_all_tickers()
            
            report = {
                'generated_at': datetime.now().isoformat(),
                'total_companies': len(all_tickers),
                'data_source_analysis': {},
                'scraping_readiness': {},
                'data_quality_summary': {},
                'integration_opportunities': []
            }
            
            # Analiza ≈∫r√≥de≈Ç danych
            by_source = {}
            with_historical = 0
            with_scraping = 0
            ready_for_integration = 0
            
            for ticker_info in all_tickers:
                source = ticker_info['data_source']
                if source not in by_source:
                    by_source[source] = []
                by_source[source].append(ticker_info['ticker'])
                
                if 'historical' in source:
                    with_historical += 1
                if 'scraping' in source:
                    with_scraping += 1
                if source == 'both':
                    ready_for_integration += 1
            
            report['data_source_analysis'] = {
                'by_source': by_source,
                'with_historical': with_historical,
                'with_scraping': with_scraping,
                'fully_integrated': ready_for_integration
            }
            
            # Analiza gotowo≈õci do scrapingu
            tickers_for_scraping = self.ticker_manager.get_tickers_for_scraping()
            high_priority = []
            medium_priority = []
            
            for ticker in tickers_for_scraping:
                suggestion = self.ticker_manager.suggest_scraping_schedule(ticker)
                if suggestion['priority'] == 'high':
                    high_priority.append(ticker)
                elif suggestion['priority'] == 'medium':
                    medium_priority.append(ticker)
            
            report['scraping_readiness'] = {
                'total_ready': len(tickers_for_scraping),
                'high_priority': high_priority,
                'medium_priority': medium_priority,
                'low_priority': len(tickers_for_scraping) - len(high_priority) - len(medium_priority)
            }
            
            # Rekomendacje integracji
            if len(high_priority) > 0:
                report['integration_opportunities'].append({
                    'type': 'immediate_scraping',
                    'tickers': high_priority[:5],  # Top 5
                    'reason': 'Tickery z danymi historycznymi ale nieaktualnymi - natychmiast rozpocznij scraping'
                })
            
            if ready_for_integration < with_historical:
                historical_only = with_historical - ready_for_integration
                report['integration_opportunities'].append({
                    'type': 'enable_scraping',
                    'count': historical_only,
                    'reason': f'{historical_only} ticker√≥w ma tylko dane historyczne - w≈ÇƒÖcz dla nich scraping'
                })
            
            logger.info(f"üìã Raport integracji: {ready_for_integration}/{len(all_tickers)} "
                       f"w pe≈Çni zintegrowanych, {len(high_priority)} wysokiej priority")
            
            return report
            
        except Exception as e:
            logger.error(f"‚ùå B≈ÇƒÖd generowania raportu integracji: {e}")
            return {}

def enhanced_auto_register_ticker_from_import(ticker: str, import_stats: Dict) -> Dict[str, Any]:
    """G≈Ç√≥wna funkcja rozszerzonej auto-rejestracji - zastƒÖpienie starej funkcji"""
    try:
        auto_registration = EnhancedTickerAutoRegistration()
        return auto_registration.auto_register_from_import(ticker, import_stats)
    except Exception as e:
        logger.error(f"‚ùå B≈ÇƒÖd enhanced auto-rejestracji dla {ticker}: {e}")
        return {
            'ticker': ticker,
            'registered': False,
            'errors': [str(e)]
        }

def main():
    """Demo rozszerzonej auto-rejestracji"""
    print("üöÄ ENHANCED TICKER AUTO-REGISTRATION SYSTEM")
    print("=" * 60)
    
    try:
        auto_reg = EnhancedTickerAutoRegistration()
        
        # Test rejestracji nowego tickera
        test_ticker = "NEWTEST"
        test_stats = {
            'imported': 1500,
            'skipped': 10,
            'errors': 2,
            'source_file': 'NEWTEST_5min.txt'
        }
        
        print(f"\nüîÑ Testujƒô rejestracjƒô: {test_ticker}")
        result = auto_reg.auto_register_from_import(test_ticker, test_stats)
        
        print(f"‚úÖ Rezultat rejestracji:")
        print(f"   Zarejestrowany: {result['registered']}")
        print(f"   Sp√≥≈Çka: {result['company_info'].get('name', 'N/A')}")
        print(f"   Sektor: {result['company_info'].get('sector', 'N/A')}")
        print(f"   ≈πr√≥d≈Ço danych: {result['data_source_registered']}")
        print(f"   Analiza ciƒÖg≈Ço≈õci: {result['continuity_analyzed']}")
        
        if result.get('scraping_suggestion'):
            suggestion = result['scraping_suggestion']
            print(f"   Rekomendacja scrapingu: {suggestion['priority']} - {suggestion['reason']}")
        
        # Generuj raport integracji
        print(f"\nüìã RAPORT GOTOWO≈öCI INTEGRACJI:")
        print("-" * 40)
        
        report = auto_reg.generate_integration_readiness_report()
        
        print(f"üìä ≈ÅƒÖczne sp√≥≈Çki: {report['total_companies']}")
        print(f"üìà Z danymi historycznymi: {report['data_source_analysis']['with_historical']}")
        print(f"üîÑ Z scrapingiem: {report['data_source_analysis']['with_scraping']}")
        print(f"üéØ W pe≈Çni zintegrowane: {report['data_source_analysis']['fully_integrated']}")
        
        scraping = report['scraping_readiness']
        print(f"\nüöÄ Gotowe do scrapingu: {scraping['total_ready']}")
        print(f"   üî• Wysokiej priority: {len(scraping['high_priority'])}")
        print(f"   ‚ö†Ô∏è ≈öredniej priority: {len(scraping['medium_priority'])}")
        
        if scraping['high_priority']:
            print(f"   Top priority: {', '.join(scraping['high_priority'][:5])}")
        
        print(f"\nüí° Mo≈ºliwo≈õci integracji: {len(report['integration_opportunities'])}")
        for opp in report['integration_opportunities']:
            print(f"   - {opp['type']}: {opp['reason']}")
        
    except Exception as e:
        print(f"‚ùå B≈ÇƒÖd g≈Ç√≥wny: {e}")

if __name__ == "__main__":
    main()
