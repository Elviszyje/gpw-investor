#!/usr/bin/env python3
"""
Scheduler dla news√≥w finansowych i komunikat√≥w ESPI/EBI
Automatyczne pobieranie artyku≈Ç√≥w i komunikat√≥w gie≈Çdowych w regularnych odstƒôpach
Autor: System GPW Investor
Data: 2025-06-24
"""

import os
import sys
import time
import threading
from datetime import datetime, timedelta
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.interval import IntervalTrigger
import logging

# Dodaj ≈õcie≈ºkƒô do modu≈Ç√≥w
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from workers.news_scraper_offline import NewsScraperOffline
from workers.espi_scraper_rss import ESPIScraperRSS

# Konfiguracja logowania
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class NewsScheduler:
    """Scheduler dla automatycznego pobierania news√≥w finansowych i komunikat√≥w ESPI/EBI"""
    
    def __init__(self):
        """Inicjalizacja schedulera"""
        self.scheduler = BackgroundScheduler()
        self.news_scraper = NewsScraperOffline()
        self.espi_scraper = ESPIScraperRSS()
        
        # Statusy
        self.is_running = False
        self.last_news_run = None
        self.last_espi_run = None
        self.last_news_results = {'total': 0, 'by_portal': {}}
        self.last_espi_results = {'ESPI': 0, 'EBI': 0, 'total': 0}
        
        # Interwa≈Çy (w minutach)
        self.news_interval_minutes = 30  # Co 30 minut news√≥w
        self.espi_interval_minutes = 60  # Co godzinƒô komunikaty ESPI/EBI
        
        # Ustawienia pobierania
        self.news_days_back = 1  # Pobieraj newsy z ostatniego dnia
        self.espi_days_back = 1  # Pobieraj komunikaty z ostatniego dnia
        
        logger.info("‚úì News Scheduler zainicjalizowany")
    
    def news_scraping_job(self):
        """Job do scrapowania news√≥w finansowych"""
        try:
            logger.info("üì∞ Rozpoczynam automatyczne scrapowanie news√≥w finansowych")
            
            # Pobierz newsy z portali finansowych
            results = self.news_scraper.scrape_all_news(days_back=self.news_days_back)
            
            self.last_news_run = datetime.now()
            
            # Oblicz total
            total_articles = sum(results.values()) if isinstance(results, dict) else 0
            
            self.last_news_results = {
                'total': total_articles,
                'by_portal': results if isinstance(results, dict) else {}
            }
            
            logger.info(f"‚úÖ Scrapowanie news√≥w zako≈Ñczone. Pobrano {total_articles} artyku≈Ç√≥w")
            
            # Log szczeg√≥≈Ç√≥w po portalach
            if isinstance(results, dict):
                for portal, count in results.items():
                    if count > 0:
                        logger.info(f"   üìä {portal}: {count} artyku≈Ç√≥w")
                    
        except Exception as e:
            logger.error(f"‚ùå B≈ÇƒÖd podczas scrapowania news√≥w: {e}")
            self.last_news_results = {'total': 0, 'by_portal': {}, 'error': str(e)}
    
    def espi_scraping_job(self):
        """Job do scrapowania komunikat√≥w ESPI/EBI"""
        try:
            logger.info("üìã Rozpoczynam automatyczne scrapowanie komunikat√≥w ESPI/EBI")
            
            # Pobierz komunikaty ESPI/EBI
            results = self.espi_scraper.scrape_all_communications(days_back=self.espi_days_back)
            
            self.last_espi_run = datetime.now()
            self.last_espi_results = results
            
            total_communications = results.get('total', 0)
            logger.info(f"‚úÖ Scrapowanie komunikat√≥w zako≈Ñczone. Pobrano {total_communications} komunikat√≥w")
            
            # Log szczeg√≥≈Ç√≥w po typach
            espi_count = results.get('ESPI', 0)
            ebi_count = results.get('EBI', 0)
            if espi_count > 0:
                logger.info(f"   üìä ESPI: {espi_count} komunikat√≥w")
            if ebi_count > 0:
                logger.info(f"   üìä EBI: {ebi_count} komunikat√≥w")
                    
        except Exception as e:
            logger.error(f"‚ùå B≈ÇƒÖd podczas scrapowania komunikat√≥w ESPI/EBI: {e}")
            self.last_espi_results = {'ESPI': 0, 'EBI': 0, 'total': 0, 'error': str(e)}
    
    def start_scraping(self):
        """Uruchom automatyczne scrapowanie"""
        if self.is_running:
            logger.warning("‚ö†Ô∏è Scheduler ju≈º dzia≈Ça")
            return False
        
        try:
            # Dodaj job dla news√≥w
            self.scheduler.add_job(
                func=self.news_scraping_job,
                trigger=IntervalTrigger(minutes=self.news_interval_minutes),
                id='news_scraping_job',
                name='Automatyczne pobieranie news√≥w finansowych',
                replace_existing=True,
                max_instances=1
            )
            
            # Dodaj job dla komunikat√≥w ESPI/EBI
            self.scheduler.add_job(
                func=self.espi_scraping_job,
                trigger=IntervalTrigger(minutes=self.espi_interval_minutes),
                id='espi_scraping_job',
                name='Automatyczne pobieranie komunikat√≥w ESPI/EBI',
                replace_existing=True,
                max_instances=1
            )
            
            self.scheduler.start()
            self.is_running = True
            
            logger.info(f"‚úÖ News Scheduler uruchomiony:")
            logger.info(f"   üì∞ Newsy: co {self.news_interval_minutes} minut")
            logger.info(f"   üìã ESPI/EBI: co {self.espi_interval_minutes} minut")
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå B≈ÇƒÖd podczas uruchamiania schedulera: {e}")
            return False
    
    def stop_scraping(self):
        """Zatrzymaj automatyczne scrapowanie"""
        if not self.is_running:
            logger.warning("‚ö†Ô∏è Scheduler nie dzia≈Ça")
            return False
        
        try:
            self.scheduler.shutdown()
            self.is_running = False
            logger.info("‚úÖ News Scheduler zatrzymany")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå B≈ÇƒÖd podczas zatrzymywania schedulera: {e}")
            return False
    
    def update_news_interval(self, new_interval_minutes: int):
        """Aktualizuj interwa≈Ç pobierania news√≥w"""
        if new_interval_minutes < 5:
            logger.warning("‚ö†Ô∏è Interwa≈Ç news√≥w nie mo≈ºe byƒá mniejszy ni≈º 5 minut")
            return False
        
        self.news_interval_minutes = new_interval_minutes
        
        if self.is_running:
            # Usu≈Ñ stary job i dodaj nowy z nowym interwa≈Çem
            self.scheduler.remove_job('news_scraping_job')
            self.scheduler.add_job(
                func=self.news_scraping_job,
                trigger=IntervalTrigger(minutes=self.news_interval_minutes),
                id='news_scraping_job',
                name='Automatyczne pobieranie news√≥w finansowych',
                replace_existing=True,
                max_instances=1
            )
        
        logger.info(f"‚úÖ Interwa≈Ç news√≥w zaktualizowany na {new_interval_minutes} minut")
        return True
    
    def update_espi_interval(self, new_interval_minutes: int):
        """Aktualizuj interwa≈Ç pobierania komunikat√≥w ESPI/EBI"""
        if new_interval_minutes < 15:
            logger.warning("‚ö†Ô∏è Interwa≈Ç ESPI/EBI nie mo≈ºe byƒá mniejszy ni≈º 15 minut")
            return False
        
        self.espi_interval_minutes = new_interval_minutes
        
        if self.is_running:
            # Usu≈Ñ stary job i dodaj nowy z nowym interwa≈Çem
            self.scheduler.remove_job('espi_scraping_job')
            self.scheduler.add_job(
                func=self.espi_scraping_job,
                trigger=IntervalTrigger(minutes=self.espi_interval_minutes),
                id='espi_scraping_job',
                name='Automatyczne pobieranie komunikat√≥w ESPI/EBI',
                replace_existing=True,
                max_instances=1
            )
        
        logger.info(f"‚úÖ Interwa≈Ç ESPI/EBI zaktualizowany na {new_interval_minutes} minut")
        return True
    
    def update_news_days_back(self, new_days_back: int):
        """Aktualizuj liczbƒô dni wstecz dla news√≥w"""
        if new_days_back < 1 or new_days_back > 7:
            logger.warning("‚ö†Ô∏è Liczba dni dla news√≥w musi byƒá miƒôdzy 1 a 7")
            return False
        
        self.news_days_back = new_days_back
        logger.info(f"‚úÖ Zakres dni dla news√≥w zaktualizowany na {new_days_back} dni")
        return True
    
    def update_espi_days_back(self, new_days_back: int):
        """Aktualizuj liczbƒô dni wstecz dla komunikat√≥w ESPI/EBI"""
        if new_days_back < 1 or new_days_back > 7:
            logger.warning("‚ö†Ô∏è Liczba dni dla ESPI/EBI musi byƒá miƒôdzy 1 a 7")
            return False
        
        self.espi_days_back = new_days_back
        logger.info(f"‚úÖ Zakres dni dla ESPI/EBI zaktualizowany na {new_days_back} dni")
        return True
    
    def run_manual_news_scrape(self):
        """Uruchom jednorazowe scrapowanie news√≥w"""
        try:
            logger.info("üîß Uruchamianie manualnego scrapowania news√≥w")
            self.news_scraping_job()
            return True
        except Exception as e:
            logger.error(f"‚ùå B≈ÇƒÖd podczas manualnego scrapowania news√≥w: {e}")
            return False
    
    def run_manual_espi_scrape(self):
        """Uruchom jednorazowe scrapowanie komunikat√≥w ESPI/EBI"""
        try:
            logger.info("üîß Uruchamianie manualnego scrapowania komunikat√≥w ESPI/EBI")
            self.espi_scraping_job()
            return True
        except Exception as e:
            logger.error(f"‚ùå B≈ÇƒÖd podczas manualnego scrapowania komunikat√≥w ESPI/EBI: {e}")
            return False
    
    def get_status(self):
        """Pobierz status schedulera"""
        jobs = []
        if self.is_running and self.scheduler.get_jobs():
            for job in self.scheduler.get_jobs():
                next_run = job.next_run_time.strftime('%Y-%m-%d %H:%M:%S') if job.next_run_time else 'N/A'
                jobs.append({
                    'id': job.id,
                    'name': job.name,
                    'next_run': next_run
                })
        
        return {
            'is_running': self.is_running,
            'jobs': jobs,
            'news_interval_minutes': self.news_interval_minutes,
            'espi_interval_minutes': self.espi_interval_minutes,
            'news_days_back': self.news_days_back,
            'espi_days_back': self.espi_days_back,
            'last_news_run': self.last_news_run.strftime('%Y-%m-%d %H:%M:%S') if self.last_news_run else None,
            'last_espi_run': self.last_espi_run.strftime('%Y-%m-%d %H:%M:%S') if self.last_espi_run else None,
            'last_news_results': self.last_news_results,
            'last_espi_results': self.last_espi_results
        }


# Globalny singleton schedulera
_news_scheduler = None

def get_news_scheduler():
    """Pobierz instancjƒô globalnego news schedulera"""
    global _news_scheduler
    if _news_scheduler is None:
        _news_scheduler = NewsScheduler()
    return _news_scheduler

def start_news_scraping():
    """Uruchom news scheduler"""
    scheduler = get_news_scheduler()
    return scheduler.start_scraping()

def stop_news_scraping():
    """Zatrzymaj news scheduler"""
    scheduler = get_news_scheduler()
    return scheduler.stop_scraping()

def get_news_scheduler_status():
    """Pobierz status news schedulera"""
    scheduler = get_news_scheduler()
    return scheduler.get_status()

def run_manual_news_scrape():
    """Uruchom manualne scrapowanie news√≥w"""
    scheduler = get_news_scheduler()
    return scheduler.run_manual_news_scrape()

def run_manual_espi_scrape():
    """Uruchom manualne scrapowanie komunikat√≥w ESPI/EBI"""
    scheduler = get_news_scheduler()
    return scheduler.run_manual_espi_scrape()

def update_news_interval(new_interval_minutes: int):
    """Aktualizuj interwa≈Ç news√≥w"""
    scheduler = get_news_scheduler()
    return scheduler.update_news_interval(new_interval_minutes)

def update_espi_interval(new_interval_minutes: int):
    """Aktualizuj interwa≈Ç ESPI/EBI"""
    scheduler = get_news_scheduler()
    return scheduler.update_espi_interval(new_interval_minutes)

def update_news_days_back(new_days_back: int):
    """Aktualizuj zakres dni dla news√≥w"""
    scheduler = get_news_scheduler()
    return scheduler.update_news_days_back(new_days_back)

def update_espi_days_back(new_days_back: int):
    """Aktualizuj zakres dni dla ESPI/EBI"""
    scheduler = get_news_scheduler()
    return scheduler.update_espi_days_back(new_days_back)


if __name__ == "__main__":
    """Test schedulera"""
    logger.info("üß™ Test News Schedulera")
    
    scheduler = NewsScheduler()
    
    # Test manualny
    logger.info("üì∞ Test manualnego scrapowania news√≥w...")
    scheduler.run_manual_news_scrape()
    
    logger.info("üìã Test manualnego scrapowania ESPI/EBI...")
    scheduler.run_manual_espi_scrape()
    
    # Status
    status = scheduler.get_status()
    logger.info(f"üìä Status: {status}")
    
    logger.info("‚úÖ Test zako≈Ñczony")
