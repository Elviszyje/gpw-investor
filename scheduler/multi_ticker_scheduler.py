#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import json
import sys
from datetime import datetime, timedelta
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.interval import IntervalTrigger

# Dodanie ≈õcie≈ºki do workers
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'workers'))

from bankier_scraper import BankierScraper


class MultiTickerScheduler:
    def __init__(self):
        self.scheduler = BackgroundScheduler()
        self.config = self.load_config()
        self.is_running = False
        
        # Inicjalizuj scraper do pobierania ticker√≥w z bazy
        try:
            settings = self.config.get('scraping_settings', {})
            self.scraper = BankierScraper(
                use_selenium=False,  # Do pobierania listy ticker√≥w nie trzeba Selenium
                headless=True
            )
        except Exception as e:
            print(f"‚ö†Ô∏è B≈ÇƒÖd inicjalizacji scrapera: {e}")
            self.scraper = None
    
    def load_config(self):
        """≈Åaduje konfiguracjƒô ticker√≥w z pliku JSON"""
        # Try multiple possible locations for tickers_config.json
        possible_paths = [
            '/app/tickers_config.json',  # Docker app root (first priority)
            os.path.join(os.getcwd(), 'tickers_config.json'),  # Current working directory
            os.path.join(os.path.dirname(__file__), '..', 'tickers_config.json'),  # Original path
        ]
        
        config = None
        config_path = None
        
        for path in possible_paths:
            if os.path.exists(path):
                config_path = path
                break
        
        if config_path:
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                    print(f"‚úÖ Za≈Çadowano konfiguracjƒô z {config_path}")
                    if 'active_tickers' in config:
                        print(f"üìã Znaleziono {len(config['active_tickers'])} aktywnych ticker√≥w")
                    return config
            except Exception as e:
                print(f"‚ùå B≈ÇƒÖd ≈Çadowania konfiguracji z {config_path}: {e}")
        
        print(f"‚ö†Ô∏è Nie znaleziono pliku tickers_config.json, u≈ºywam domy≈õlnej konfiguracji")
        # Domy≈õlna konfiguracja
        return {
            "active_tickers": ["PKN", "CDR", "PKO"],
            "scraping_settings": {
                "interval_minutes": 15,
                "use_selenium": True,
                    "headless": True,
                    "max_retries": 3
                }
            }
    
    def save_config(self):
        """Zapisuje konfiguracjƒô do pliku JSON"""
        # Try multiple possible locations for tickers_config.json
        possible_paths = [
            '/app/tickers_config.json',  # Docker app root (first priority)
            os.path.join(os.getcwd(), 'tickers_config.json'),  # Current working directory
            os.path.join(os.path.dirname(__file__), '..', 'tickers_config.json'),  # Original path
        ]
        
        # Try to save to first writable location
        for config_path in possible_paths:
            try:
                # Create directory if it doesn't exist
                os.makedirs(os.path.dirname(config_path), exist_ok=True)
                
                with open(config_path, 'w', encoding='utf-8') as f:
                    json.dump(self.config, f, indent=4, ensure_ascii=False)
                    print(f"‚úÖ Konfiguracja zapisana pomy≈õlnie do {config_path}")
                    return True
            except Exception as e:
                print(f"‚ö†Ô∏è Nie mo≈ºna zapisaƒá do {config_path}: {e}")
                continue
        
        print(f"‚ùå Nie mo≈ºna zapisaƒá konfiguracji do ≈ºadnej z lokalizacji")
        return False
    
    def add_ticker(self, ticker):
        """Dodaje ticker do listy aktywnych - modyfikuje bazƒô danych"""
        ticker = ticker.upper()
        
        # Je≈õli mamy dostƒôp do bazy danych, u≈ºywaj jej
        if self.scraper and hasattr(self.scraper, 'engine') and self.scraper.engine:
            try:
                from sqlalchemy import text
                from datetime import datetime
                
                with self.scraper.engine.connect() as conn:
                    # Sprawd≈∫ czy ticker ju≈º istnieje
                    result = conn.execute(text(
                        "SELECT ticker, is_active FROM ticker_mappings WHERE ticker = :ticker"
                    ), {"ticker": ticker})
                    
                    existing = result.fetchone()
                    
                    if existing:
                        # Ticker istnieje - aktywuj go je≈õli nieaktywny
                        if not existing[1]:  # is_active = False
                            conn.execute(text("""
                                UPDATE ticker_mappings 
                                SET is_active = true, updated_at = :updated_at 
                                WHERE ticker = :ticker
                            """), {
                                'ticker': ticker,
                                'updated_at': datetime.utcnow()
                            })
                            conn.commit()
                            print(f"Aktywowano istniejƒÖcy ticker: {ticker}")
                            return True
                        else:
                            print(f"Ticker {ticker} ju≈º jest aktywny")
                            return False
                    else:
                        # Ticker nie istnieje - dodaj nowy
                        conn.execute(text("""
                            INSERT INTO ticker_mappings (ticker, bankier_symbol, description, is_active, created_at, updated_at)
                            VALUES (:ticker, :bankier_symbol, :description, :is_active, :created_at, :updated_at)
                        """), {
                            'ticker': ticker,
                            'bankier_symbol': ticker,  # Domy≈õlnie symbol = ticker
                            'description': f'Automatycznie dodany ticker {ticker}',
                            'is_active': True,
                            'created_at': datetime.utcnow(),
                            'updated_at': datetime.utcnow()
                        })
                        conn.commit()
                        print(f"Dodano nowy ticker do bazy: {ticker}")
                        return True
                        
            except Exception as e:
                print(f"B≈ÇƒÖd podczas dodawania tickera do bazy: {e}")
                # Fallback do pliku JSON
                pass
        
        # Fallback - u≈ºyj pliku JSON
        if ticker not in self.config['active_tickers']:
            self.config['active_tickers'].append(ticker)
            self.save_config()
            print(f"Dodano ticker do pliku JSON: {ticker}")
            return True
        else:
            print(f"Ticker {ticker} ju≈º istnieje w pliku JSON")
            return False
    
    def remove_ticker(self, ticker):
        """Usuwa ticker z listy aktywnych - modyfikuje bazƒô danych"""
        ticker = ticker.upper()
        
        # Je≈õli mamy dostƒôp do bazy danych, u≈ºywaj jej
        if self.scraper and hasattr(self.scraper, 'engine') and self.scraper.engine:
            try:
                from sqlalchemy import text
                from datetime import datetime
                
                with self.scraper.engine.connect() as conn:
                    # Sprawd≈∫ czy ticker istnieje i jest aktywny
                    result = conn.execute(text(
                        "SELECT ticker, is_active FROM ticker_mappings WHERE ticker = :ticker"
                    ), {"ticker": ticker})
                    
                    existing = result.fetchone()
                    
                    if existing and existing[1]:  # istnieje i is_active = True
                        # Deaktywuj ticker (nie usuwaj, ≈ºeby zachowaƒá dane historyczne)
                        conn.execute(text("""
                            UPDATE ticker_mappings 
                            SET is_active = false, updated_at = :updated_at 
                            WHERE ticker = :ticker
                        """), {
                            'ticker': ticker,
                            'updated_at': datetime.utcnow()
                        })
                        conn.commit()
                        print(f"Deaktywowano ticker w bazie: {ticker}")
                        return True
                    elif existing and not existing[1]:
                        print(f"Ticker {ticker} ju≈º jest nieaktywny")
                        return False
                    else:
                        print(f"Ticker {ticker} nie istnieje w bazie")
                        return False
                        
            except Exception as e:
                print(f"B≈ÇƒÖd podczas usuwania tickera z bazy: {e}")
                # Fallback do pliku JSON
                pass
        
        # Fallback - u≈ºyj pliku JSON
        if ticker in self.config['active_tickers']:
            self.config['active_tickers'].remove(ticker)
            self.save_config()
            print(f"Usuniƒôto ticker z pliku JSON: {ticker}")
            return True
        else:
            print(f"Ticker {ticker} nie istnieje w pliku JSON")
            return False
    
    def get_active_tickers(self):
        """Pobiera listƒô aktywnych ticker√≥w"""
        # Spr√≥buj pobraƒá z bazy danych przez scraper
        if self.scraper:
            try:
                db_tickers = self.scraper.get_active_tickers()
                if db_tickers:
                    print(f"‚úì Pobrano {len(db_tickers)} ticker√≥w z bazy danych")
                    return db_tickers
            except Exception as e:
                print(f"‚ö†Ô∏è B≈ÇƒÖd pobierania ticker√≥w z bazy: {e}")
        
        # Fallback - u≈ºyj konfiguracji z pliku
        return self.config.get('active_tickers', [])
    
    def update_scraping_interval(self, minutes):
        """Aktualizuje interwa≈Ç scrapowania"""
        self.config['scraping_settings']['interval_minutes'] = minutes
        self.save_config()
        
        # Restart schedulera je≈õli jest uruchomiony
        if self.is_running:
            success = self.restart()
            if success:
                print(f"Zaktualizowano interwa≈Ç na {minutes} minut i restartowano scheduler")
            else:
                print(f"Zaktualizowano interwa≈Ç na {minutes} minut ale b≈ÇƒÖd restartu schedulera")
        else:
            print(f"Zaktualizowano interwa≈Ç na {minutes} minut")
    
    def scrape_job(self):
        """Funkcja wykonywana przez scheduler - scrapuje wszystkie aktywne tickery"""
        try:
            # Sprawd≈∫ czy scheduler nie zosta≈Ç zatrzymany w miƒôdzyczasie
            if not self.is_running:
                print("Scheduler zosta≈Ç zatrzymany - anulowanie zadania")
                return
            
            print(f"\n=== Cykliczne scrapowanie - {datetime.now()} ===")
            
            active_tickers = self.get_active_tickers()
            
            if not active_tickers:
                print("Brak aktywnych ticker√≥w do scrapowania")
                return
            
            # Sprawd≈∫ ponownie czy scheduler nie zosta≈Ç zatrzymany
            if not self.is_running:
                print("Scheduler zosta≈Ç zatrzymany podczas pobierania ticker√≥w - anulowanie")
                return
            
            # Inicjalizacja scrapera do scrapowania (z Selenium) je≈õli potrzeba
            settings = self.config['scraping_settings']
            
            # Sprawd≈∫ czy scraper istnieje i jest sprawny
            if not hasattr(self, 'scraping_scraper') or not self.scraping_scraper:
                if not self.is_running:  # Sprawd≈∫ przed inicjalizacjƒÖ
                    return
                    
                try:
                    self.scraping_scraper = BankierScraper(
                        use_selenium=settings.get('use_selenium', True),
                        headless=settings.get('headless', True)
                    )
                except Exception as e:
                    print(f"B≈ÇƒÖd inicjalizacji scrapera: {e}")
                    return
            
            # Ostatnie sprawdzenie przed scrapowaniem
            if not self.is_running:
                print("Scheduler zatrzymany przed rozpoczƒôciem scrapowania")
                return
            
            results = self.scraping_scraper.scrape_multiple_tickers(active_tickers)
            
            # Sprawd≈∫ czy nadal dzia≈Çamy przed wypisaniem wynik√≥w
            if self.is_running:
                print(f"Cykliczne scrapowanie zako≈Ñczone:")
                print(f"  Udane: {len(results['success'])} ticker√≥w")
                print(f"  Nieudane: {len(results['failed'])} ticker√≥w")
                
                if results['failed']:
                    print(f"  B≈Çƒôdy dla: {', '.join(results['failed'])}")
                
        except Exception as e:
            print(f"B≈ÇƒÖd podczas cyklicznego scrapowania: {e}")
            
            # W przypadku powa≈ºnego b≈Çƒôdu, spr√≥buj zrestartowaƒá scraper
            if hasattr(self, 'scraping_scraper') and self.scraping_scraper:
                try:
                    self.scraping_scraper.close()
                    self.scraping_scraper = None
                except:
                    pass
    
    def start(self):
        """Uruchamia cykliczne scrapowanie"""
        if self.is_running:
            print("Scheduler ju≈º jest uruchomiony")
            return False
        
        try:
            # Bezpieczne zamkniƒôcie poprzedniego schedulera je≈õli istnieje
            if hasattr(self, 'scheduler') and self.scheduler:
                try:
                    if self.scheduler.running:
                        self.scheduler.shutdown(wait=False)
                except:
                    pass
            
            # Utworzenie nowego schedulera
            self.scheduler = BackgroundScheduler()
            
            interval_minutes = self.config['scraping_settings']['interval_minutes']
            
            # Sprawdzenie czy interwa≈Ç nie jest zbyt kr√≥tki
            if interval_minutes < 3:
                print(f"‚ö†Ô∏è Ostrze≈ºenie: Interwa≈Ç {interval_minutes} min mo≈ºe byƒá zbyt kr√≥tki. Zalecane minimum: 3 min")
            
            # Dodanie zadania do schedulera
            self.scheduler.add_job(
                func=self.scrape_job,
                trigger=IntervalTrigger(minutes=interval_minutes),
                id='multi_ticker_scrape',
                name='Multi Ticker Scraping',
                replace_existing=True,
                max_instances=1,  # Tylko jedna instancja na raz
                coalesce=True,    # Je≈õli zadanie siƒô op√≥≈∫ni, nie uruchamiaj kolejnego
                misfire_grace_time=30  # 30 sekund tolerancji na op√≥≈∫nienie
            )
            
            # Uruchomienie schedulera
            self.scheduler.start()
            self.is_running = True
            
            print(f"Cykliczne scrapowanie uruchomione (interwa≈Ç: {interval_minutes} min)")
            print(f"Aktywne tickery: {', '.join(self.get_active_tickers())}")
            
            return True
            
        except Exception as e:
            print(f"B≈ÇƒÖd podczas uruchamiania schedulera: {e}")
            self.is_running = False
            return False
    
    def stop(self):
        """Zatrzymuje cykliczne scrapowanie"""
        if not self.is_running:
            print("Scheduler nie jest uruchomiony")
            return False
        
        try:
            # Oznacz jako zatrzymany przed shutdown
            self.is_running = False
            
            # Usu≈Ñ wszystkie zadania przed shutdown
            if hasattr(self, 'scheduler') and self.scheduler:
                if self.scheduler.running:
                    try:
                        self.scheduler.remove_all_jobs()
                    except:
                        pass
                    
                    # Zatrzymaj scheduler bez oczekiwania na zadania
                    try:
                        self.scheduler.shutdown(wait=False)
                    except:
                        pass
            
            # Zamkniƒôcie scraperow
            if hasattr(self, 'scraping_scraper') and self.scraping_scraper:
                try:
                    self.scraping_scraper.close()
                except:
                    pass
                self.scraping_scraper = None
                
            if self.scraper:
                try:
                    self.scraper.close()
                except:
                    pass
                self.scraper = None
            
            print("Cykliczne scrapowanie zatrzymane")
            return True
            
        except Exception as e:
            print(f"B≈ÇƒÖd podczas zatrzymywania schedulera: {e}")
            self.is_running = False
            return False
    
    def restart(self):
        """Bezpieczny restart schedulera"""
        try:
            print("Restartowanie schedulera...")
            was_running = self.is_running
            
            if was_running:
                self.stop()
                
            # Kr√≥tka przerwa ≈ºeby wszystko siƒô wyczy≈õci≈Ço
            import time
            time.sleep(2)
            
            if was_running:
                return self.start()
            else:
                print("Scheduler nie by≈Ç uruchomiony - brak restartu")
                return True
                
        except Exception as e:
            print(f"B≈ÇƒÖd podczas restartu schedulera: {e}")
            return False
    
    def safe_shutdown(self):
        """Bezpieczne zamkniƒôcie schedulera z obs≈ÇugƒÖ b≈Çƒôd√≥w"""
        try:
            if self.is_running and self.scheduler.running:
                # Usu≈Ñ wszystkie zadania
                self.scheduler.remove_all_jobs()
                
                # Zatrzymaj scheduler
                self.scheduler.shutdown(wait=True)
                
            self.is_running = False
            
            # Zamknij wszystkie scrapery
            if hasattr(self, 'scraping_scraper') and self.scraping_scraper:
                try:
                    self.scraping_scraper.close()
                except:
                    pass
                self.scraping_scraper = None
                
            if self.scraper:
                try:
                    self.scraper.close()
                except:
                    pass
                self.scraper = None
                
            print("Scheduler bezpiecznie zamkniƒôty")
            return True
            
        except Exception as e:
            print(f"B≈ÇƒÖd podczas bezpiecznego zamykania: {e}")
            self.is_running = False
            return False
    
    def status(self):
        """Zwraca status schedulera"""
        return {
            'is_running': self.is_running,
            'active_tickers': self.get_active_tickers(),
            'interval_minutes': self.config['scraping_settings']['interval_minutes'],
            'use_selenium': self.config['scraping_settings']['use_selenium'],
            'next_run': None if not self.is_running else 'N/A'  # APScheduler nie udostƒôpnia ≈Çatwo next_run
        }
    
    def run_manual_scrape(self, tickers=None):
        """Uruchamia rƒôczne scrapowanie dla wybranych ticker√≥w"""
        if tickers is None:
            tickers = self.get_active_tickers()
        
        if not tickers:
            print("Brak ticker√≥w do scrapowania")
            return {'success': [], 'failed': []}
        
        print(f"Rƒôczne scrapowanie ticker√≥w: {', '.join(tickers)}")
        
        # Tymczasowy scraper dla rƒôcznego scrapowania
        settings = self.config['scraping_settings']
        temp_scraper = BankierScraper(
            use_selenium=settings.get('use_selenium', True),
            headless=settings.get('headless', True)
        )
        
        try:
            results = temp_scraper.scrape_multiple_tickers(tickers)
            return results
        finally:
            temp_scraper.close()


# Globalna instancja schedulera
multi_scheduler = MultiTickerScheduler()


def get_multi_scheduler():
    """Zwraca globalnƒÖ instancjƒô schedulera"""
    return multi_scheduler


def start_multi_ticker_scraping():
    """Uruchamia cykliczne scrapowanie wielu ticker√≥w"""
    return multi_scheduler.start()


def stop_multi_ticker_scraping():
    """Zatrzymuje cykliczne scrapowanie"""
    return multi_scheduler.stop()


def get_scheduler_status():
    """Zwraca status schedulera"""
    return multi_scheduler.status()


def run_manual_multi_scrape(tickers=None):
    """Uruchamia rƒôczne scrapowanie"""
    return multi_scheduler.run_manual_scrape(tickers)


if __name__ == "__main__":
    # Test schedulera
    print("=== Test Multi Ticker Scheduler ===")
    
    scheduler = MultiTickerScheduler()
    
    # Wy≈õwietlenie statusu
    status = scheduler.status()
    print(f"Status: {status}")
    
    # Test rƒôcznego scrapowania
    print("\nTest rƒôcznego scrapowania 2 ticker√≥w...")
    results = scheduler.run_manual_scrape(['PKN', 'CDR'])
    print(f"Wyniki: {results}")
    
    print("\nTest zako≈Ñczony")
